{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "softmax回归"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from d2l import torch as d2l\n",
    "from IPython import display\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "\n",
    "def get_dataloader_workers():\n",
    "    return 0\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0,transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"01_data/01_DataSet_FashionMNIST\",train=True,transform=trans,download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"01_data/01_DataSet_FashionMNIST\",train=False,transform=trans,download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()),\n",
    "           data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()))\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "w = torch.normal(0,0.01,size=(num_inputs,num_outputs),requires_grad=True)  # 用正态分布采样进行初始化\n",
    "b = torch.zeros(num_outputs,requires_grad=True)  # 初始为0\n",
    "\n",
    "# softmax函数\n",
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1,keepdim=True)  # 对 X_exp 的每一行进行求和操作，并保持结果的维度和原始数据相同。这样做是为了计算每个样本的和，而不是对整个数据集进行求和。\n",
    "    return X_exp / partition\n",
    "\n",
    "# 实现 softmax回归\n",
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape((-1,w.shape[1])),w)+b)  # 得分用 softmax函数处理\n",
    "\n",
    "# 交叉熵损失函数，其中 y_hat是模型的预测结果,y是真实标签\n",
    "def cross_entropy(y_hat,y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)),y])\n",
    "\n",
    "# 准确率函数\n",
    "def accuracy(y_hat,y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)  # 取预测中概率最高的类别\n",
    "    cmp = y_hat.type(y.dtype) == y  # 创建一个布尔类型的张量 cmp，用来表示预测结果与真实标签是否相等。将 y_hat转换为与 y相同数据类型后进行比较\n",
    "    return float(cmp.type(y.dtype).sum())  # 返回正确率\n",
    "\n",
    "# 准确率评估函数\n",
    "def evaluate_accuracy(net,data_iter,loss,updater):\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式，这是为了确保模型不进行梯度计算，并且不会对模型的参数进行更新\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)  # 计算损失\n",
    "        if isinstance(updater, torch.optim.Optimizer):  # 如果 updater是 pytorch的优化器的话\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()  # 这里对loss取了平均值出来\n",
    "            updater.step()\n",
    "            metric.add(float(l) * len(y), accuracy(y_hat, y), y.size().numel())  # 总的训练损失、样本正确数、样本总数\n",
    "        else:  # 如果 updater不是 pytorch的优化器，表示不需要更新模型的参数，那么代码会对损失l求和，并通过调用updater函数来更新其他参数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "\n",
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0, 0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]  # zip函数把两个列表第一个位置元素打包、第二个位置元素打包....\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 训练函数\n",
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train() # 开启训练模式\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat,y) # 计算损失\n",
    "        if isinstance(updater, torch.optim.Optimizer): # 如果updater是pytorch的优化器的话\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()  # 这里对loss取了平均值出来\n",
    "            updater.step()\n",
    "            metric.add(float(l)*len(y),accuracy(y_hat,y),y.size().numel()) # 总的训练损失、样本正确数、样本总数\n",
    "        else:\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()),accuracy(y_hat,y),y.numel())\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]\n",
    "\n",
    "# 动画绘制\n",
    "class Animator:\n",
    "    def __int__(self,x_label=None,y_label=None,legend=None,\n",
    "                x_lim=None,y_lim=None,x_scale='linear',y_scale='linear',\n",
    "                fmts=('-','m--','g-','r:'),n_rols=1,n_cols=1,figsize=(3.5,2.5)):\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        d2l.use_svg_display()\n",
    "        self.fig, self.axis = d2l.plt.subplots(n_rols,n_cols,figsize=figsize)\n",
    "        if n_rols * n_cols == 1:\n",
    "            self.axis = [self.axes,]\n",
    "        self.config_axes = lambda: d2l.set_axes(self.axes[0],x_label,y_label,x_lim,y_lim,x_scale,y_scale,legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a,b) in enumerate(zip(x,y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "# 总训练函数\n",
    "def train_ch3(net,train_iter,test_iter,loss,num_epochs,updater):\n",
    "    animator = Animator(xlabel='epoch',xlim=[1,num_epochs],ylim=[0.3,0.9],\n",
    "                       legend=['train loss','train acc','test acc'])\n",
    "    for epoch in range(num_epochs):  # 变量num_epochs遍数据\n",
    "        train_metrics = train_epoch_ch3(net,train_iter,loss,updater) # 返回两个值，一个总损失、一个总正确率\n",
    "        test_acc = evaluate_accuracy(net, test_iter) # 测试数据集上评估精度，仅返回一个值，总正确率\n",
    "        animator.add(epoch+1,train_metrics+(test_acc,)) # train_metrics+(test_acc,) 仅将两个值的正确率相加，\n",
    "    train_loss, train_acc = train_metrics\n",
    "\n",
    "# 小批量随即梯度下降来优化模型的损失函数\n",
    "lr = 0.1\n",
    "def updater(batch_size):\n",
    "    return d2l.sgd([w,b],lr,batch_size)\n",
    "\n",
    "num_epochs = 10\n",
    "train_ch3(net,train_iter,test_iter,cross_entropy,num_epochs,updater)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "softmax回归（使用框架）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "# Softmax回归的输出是一个全连接层，PyTorch不会隐式地调整输入的形状\n",
    "# 因此，定义展平层(flatten)在线性层前调整网络输入的形状\n",
    "net = nn.Sequential(nn.Flatten(),nn.Linear(784,10))\n",
    "\n",
    "# 初始化权重的函数\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,std=0.01)  # 使用正态分布来初始化该层的权重，均值为 0，方差为 0.01\n",
    "\n",
    "net.apply(init_weights)\n",
    "print(net.apply(init_weights))\n",
    "\n",
    "# 交叉熵损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 使用学习率为0.1的小批量随即梯度下降作为优化算法\n",
    "trainer = torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)\n",
    "# 绘图\n",
    "d2l.plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
