{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "二维卷积层操作（使用自定义）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1,X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+h, j:j+w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "\n",
    "X = torch.tensor([[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]])\n",
    "K = torch.tensor([[0.0,1.0],[2.0,3.0]])\n",
    "\n",
    "print(corr2d(X,K))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "实现二维卷积层"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1,X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+h, j:j+w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __int__(self, kernel_size):\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return corr2d(X, self.weight) + self.bias\n",
    "\n",
    "X = torch.ones((6,8))\n",
    "X[:,2:6] = 0  # 把中间四列设为0\n",
    "print(X)\n",
    "\n",
    "K = torch.tensor([[1.0,-2.0]])  # 设置kernel\n",
    "Y = corr2d(X,K)\n",
    "print(Y)\n",
    "print(corr2d(X.t(), K))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "学习由X生成Y的卷积核"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(1,2), bias=False) # 单个矩阵，输入通道为1，黑白图片通道为1，彩色图片通道为3。这里输入通道为1，输出通道为1.\n",
    "X = X.reshape((1,1,6,8)) # 通道维：通道数，RGB图3通道，灰度图1通道，批量维就是样本维，就是样本数\n",
    "Y = Y.reshape((1,1,6,7))\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad # 3e-2是学习率\n",
    "    if(i+1) % 2 == 0:\n",
    "        print(f'batch {i+1},loss {l.sum():.3f}')\n",
    "\n",
    "# 所学的卷积核的权重张量\n",
    "print(conv2d.weight.data.reshape((1,2)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "填充和步幅（使用框架）\n",
    "填充：在输出周围额外添加行和列，一般选取奇数卷积核，padding（填充的行数和列数）=（k-1）/2；\n",
    "     k为偶数时，p为浮点数，所做的操作为一个为向上取整，填充，一个为向下取整，填充。\n",
    "步幅：每次滑动核窗口时的行/列的步长，可以成倍减少输出形状"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 在所有侧边填充1个像素\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def comp_conv2d(conv2d, X):  # conv2d 作为传参传进去，在内部使用\n",
    "    X = X.reshape((1,1)+X.shape)  # 在维度前面加入一个通道数和批量大小数\n",
    "    Y = conv2d(X)  # 卷积处理是一个四维的矩阵\n",
    "    return Y.reshape(Y.shape[2:])  # 将前面两个维度拿掉\n",
    "\n",
    "\n",
    "conv2d = nn.Conv2d(1,1,kernel_size=3,padding=1) # padding=1 为左右都填充一行\n",
    "X = torch.rand(size=(8,8))\n",
    "print(comp_conv2d(conv2d,X).shape)\n",
    "conv2d = nn.Conv2d(1,1,kernel_size=(5,3),padding=(2,1))\n",
    "print(comp_conv2d(conv2d,X).shape)\n",
    "\n",
    "# 将高度和宽度的步幅设置为2\n",
    "conv2d = nn.Conv2d(1,1,kernel_size=3,padding=1,stride=2)\n",
    "print(comp_conv2d(conv2d,X).shape)\n",
    "\n",
    "# 一个稍微复杂的例子\n",
    "conv2d = nn.Conv2d(1,1,kernel_size=(3,5),padding=(0,1),stride=(3,4))\n",
    "print(comp_conv2d(conv2d,X).shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "多输入和多输出频道\n",
    "彩色图像有RGB三个通道，转换为灰度会丢失信息"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 多通道输入运算\n",
    "def corr2d_multi_in(X,K):\n",
    "    return sum(d2l.corr2d(x,k) for x,k in zip(X,K))\n",
    "\n",
    "X = torch.tensor([[[0.0,1.0,2.0],[3.0,4.0,5.0],[6.0,7.0,8.0]],\n",
    "                  [[1.0,2.0,3.0],[4.0,5.0,6.0],[7.0,8.0,9.0]]])\n",
    "K = torch.tensor([[[0.0,1.0],[2.0,3.0]],[[1.0,2.0],[3.0,4.0]]])\n",
    "print(corr2d_multi_in(X,K))\n",
    "\n",
    "\n",
    "# 多输出通道运算\n",
    "def corr2d_multi_in_out(X,K):  # X为3通道矩阵，K为4通道矩阵，最外面维为输出通道\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K],0)  # 大 k中每个小 k是一个3D的 Tensor。0表示 stack堆叠函数里面在 0这个维度堆叠。\n",
    "\n",
    "\n",
    "print(K.shape)\n",
    "print((K+1).shape)\n",
    "print((K+2).shape)\n",
    "print(K)\n",
    "print(K+1)\n",
    "K = torch.stack((K, K+1, K+2),0) # K与K+1之间的区别为K的每个元素加1\n",
    "print(K.shape)\n",
    "print(corr2d_multi_in_out(X,K))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
